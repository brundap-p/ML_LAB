from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Load the Iris dataset
data = load_iris()
print(data)

# Create a pandas DataFrame
df = pd.DataFrame(data.data, columns=data.feature_names)
print(df)

# Display information about the DataFrame
df.info()

# Display the first 5 rows
df.head()

# Display the last 5 rows
df.tail()

# Display descriptive statistics
df.describe

# Check for missing values
print(df.isna().sum())

# Check for null values
print(df.isnull().sum())

# Display the shape of the DataFrame
print(df.shape)

# Prepare data for clustering
X = pd.DataFrame(iris.data, columns=iris.feature_names)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Grid search for best eps and min_samples
best_score = -1
best_params = {}
best_labels = None

for eps in [0.3, 0.4, 0.5, 0.6, 0.7]:
    for min_samples in [3, 4, 5, 6]:
        model = DBSCAN(eps=eps, min_samples=min_samples)
        labels = model.fit_predict(X_scaled)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

        if n_clusters > 1:
            score = silhouette_score(X_scaled, labels)
            if score > best_score:
                best_score = score
                best_params = {'eps': eps, 'min_samples': min_samples}
                best_labels = labels

print("Best parameters:", best_params)
print(f"Best Silhouette Score: {best_score:.2f}")
print(f"Number of clusters: {len(set(best_labels)) - (1 if -1 in best_labels else 0)}")
print(f"Number of noise points: {list(best_labels).count(-1)}")

# Plotting
df_plot = pd.DataFrame(X_scaled, columns=iris.feature_names)
df_plot['Cluster'] = best_labels

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_plot, x=iris.feature_names[0], y=iris.feature_names[1], hue='Cluster', palette='Set1')
plt.title("Tuned DBSCAN Clustering on Iris Dataset")
plt.grid(True)
plt.show()
